\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{grffile}
\usepackage{amsfonts}
\usepackage{indentfirst}
\setlength{\parindent}{2em}
\usepackage[hidelinks]{hyperref}
\hypersetup{
  colorlinks   = true, %Colours links instead of ugly boxes
  urlcolor     = blue, %Colour for external hyperlinks
  linkcolor    = blue, %Colour of internal links
  citecolor   = red %Colour of citations
}
\usepackage{listings}

\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays

\title{Cyber Security Project: Repair BadNets by Neuron Pruning}
\author{author}
\date{December 2020}

\begin{document}

\maketitle

\section{Environment and how to run}
Please check the Readme.md in the  \href{https://github.com/zjzsliyang/CSAW-HackML-2020}{github repo}. We introduce the way to run the code correctly.

\section{Model Structure}
    The process of our solution can be divided into two parts. Detecting backdoors and Repair BadNets.
    Our model is based on the default model in the origin repo. See details in picture 1. 
\begin{figure}
    \centering
    \includegraphics[width=10cm,  height=6cm]{"model_structure.png"}
    \caption{Model Structure}
    \label{fig:my_label}
\end{figure}

\section{Detecting Backdoors}
This method is based on the paper \href{https://sites.cs.ucsb.edu/~bolunwang/assets/docs/backdoor-sp19.pdf}{Neural Cleanse: Identifying and Mitigating
Backdoor Attacks in Neural Networks}. We will skip the proof in this document and focus on the design ideas. 

The key point on detecting backdoors is that if a model is poisoned, it requires much smaller modifications to cause the model to classify the wrong target label. So we decided to iterate all possible labels and check which one requires smaller modification to achieve the wrong result. The whole process will be divided into 3 steps: 

\begin{enumerate}
    \item Find the minimal trigger. We try to find a trigger window with a fixed label. We assume this label is the target label of the attack backdoor trigger. The performance of this trigger depends on how small it is to misclassify all samples from other labels into the target label.
    
    \item Iterate the whole label sets. We run the loop for iterating all labels in the model, which is 1283 in our project. In other words, 1283 potential triggers will be created after this step.
    
    \item Choose the valid trigger. We need to choose the valid trigger in all 1283 triggers. It depends on the number of pixels the trigger trying to influence in the models. Our method is to calculate the L1 norms of all triggers. Then we will calculate the absolute deviation between all data points and the median. If the absolute deviation of a data point divided by that median is larger than 2, we mark it as a target trigger. The target trigger which is most effective to misclassify the model will be the 'reverse trigger' we need to repair BadNets.
\end{enumerate}

The implementation of the step 1 and 2 is in the \href{https://github.com/zjzsliyang/CSAW-HackML-2020/blob/master/visualizer.py}{\texttt{visualizer.py}}

The implementation of the step 3 is in the \href{https://github.com/zjzsliyang/CSAW-HackML-2020/blob/master/mad_outlier_detection.py}{\texttt{mad\_outlier\_detection.py}}.


\section{Repair BadNets}


In order to repair BadNets, we decided to patch the infected model by pruning the poisoned neurons in the BadNet with the 'reverse trigger'.

The target trigger poisoned neurons in the model to make it misclassify the label, so we need to find these neurons and set their output value to 0 so that the model will not be affected by the trigger anymore. 

Therefore we rank the neurons by differences between clean input and poisoned input produced by the 'reverse triggers'. We again target the second to last layer, and prune neurons by order of highest rank first. In order to keep the performance of the model on clean target, we decided to stop the iteration as soon as the model is not sensitive to the poisoned input any more.

You can find details in the \href{https://github.com/zjzsliyang/CSAW-HackML-2020/blob/master/repair_model.py}{\texttt{repair\_model.py}}.




\newpage



\end{document}

