%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Title: SOP LaTeX Template
%% Author: Soonho Kong / soonhok@cs.cmu.edu
%% Created: 2012-11-12
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Requirement:
%%     You need to have the `Adobe Caslon Pro` font family.
%%     For more information, please visit:
%%     http://store1.adobe.com/cfusion/store/html/index.cfm?store=OLS-US&event=displayFontPackage&code=1712
%%
%% How to Compile:
%%     $ xelatex main.tex
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[letterpaper]{article}
\usepackage[letterpaper, nohe adfoot]{geometry}
\usepackage{fontspec, color, enumerate, sectsty}
\usepackage[normalem]{ulem}
\usepackage{natbib}
\usepackage{url}
\usepackage{todonotes}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage{float}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{algpseudocode}
\usepackage{algorithm}
\lstset{basicstyle=\ttfamily,
  showstringspaces=false,
  commentstyle=\color{red},
  keywordstyle=\color{blue}
}
\usepackage[htt]{hyphenat}
\definecolor{wildstrawberry}{rgb}{1.0, 0.26, 0.64}
\setlength\parindent{0pt}
\linespread{1.2}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                      YOUR INFORMATION
%
%      PLEASE EDIT THE FOLLOWING LINES ACCORDINGLY!!
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\soptitle}{Cyber Security Project Report}
\newcommand{\subtitle}{CSAW-HackML-2020}
\newcommand{\yourname}{Yang Li, Yuwen Liu, Xiaofeng Xu}
\newcommand{\youremail}{yl7014@nyu.edu, yl6927@nyu.edu, xx963@nyu.edu}

%% FONTS SETUP
\defaultfontfeatures{Mapping=tex-text}
\setromanfont[Path = fonts/, Ligatures={Common}]{adobe_caslon_pro}
\setmonofont[Path = fonts/, Scale=0.8]{monaco}
\setsansfont[Path = fonts/, Scale=0.9]{Optima-Regular}
\newcommand{\amper}{{\fontspec[Scale=.95]{Adobe Caslon Pro}\selectfont\itshape\&~{}}}
\usepackage[bookmarks, colorlinks, breaklinks,
pdftitle={\yourname - \soptitle}, pdfauthor={\yourname}, unicode, colorlinks=False, hidelinks]{hyperref}
\hypersetup{linkcolor=magneta,citecolor=magenta,filecolor=magenta,urlcolor=wildstrawberry}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                      Title and Author Name
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\begin{center}{\huge \scshape \soptitle}\end{center}
\begin{center}{\large \subtitle}\end{center}
\begin{center}\vspace{0.2em} {\Large \yourname\\}
  {\youremail}\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                      SOP Body
% NOTE: Use \amper instead of \&
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Environment Setup}
Please check the \texttt{README} in the GitHub repository\footnotemark\footnotetext{\url{https://github.com/zjzsliyang/CSAW-HackML-2020}}. We introduce the dependencies as well as the bash commands to run the code.

\section*{Base Model Structure}
We implemented two approaches to detect and repair backdoored model. First is based on by \cite{wang2019neural} and second is based on \cite{gao2019strip}. Both approaches can be divided into two parts separately, e.g. detecting the backdoor labels and repair the BadNets. Our models are based on the default model in the origin repo, as following Fig. \ref{fig:base_structure} shows. 
\begin{figure}[H]
    \centering
    \includegraphics[width=10cm,  height=6cm]{img/model_structure.png}
    \caption{Model Structure}
    \label{fig:base_structure}
\end{figure}

\section*{Neural Cleanse}
\subsection*{Detect Backdoors}
The key idea of Neural Cleanse\footnotemark\footnotetext{\url{https://sites.cs.ucsb.edu/~bolunwang/assets/docs/backdoor-sp19.pdf}} by  \cite{wang2019neural} is that if a model is poisoned, it requires much smaller modifications to cause the model to classify the wrong target label. So we decided to iterate all possible labels and check which one requires smaller modification to achieve the wrong result. The whole process will be divided into 3 steps:

\begin{enumerate}
    \item Find the minimal trigger. We try to find a trigger window with a fixed label. We assume this label is the target label of the attack backdoor trigger. The performance of this trigger depends on how small it is to misclassify all samples from other labels into the target label.
    
    \item Iterate the whole label sets. We run the loop for iterating all labels in the model, which is 1283 in our project. In other words, 1283 potential triggers will be created after this step.
    
    \item Choose the valid trigger. We need to choose the valid trigger in all 1283 triggers. It depends on the number of pixels the trigger trying to influence in the models. Our method is to calculate the L1 norms of all triggers. Then we will calculate the absolute deviation between all data points and the median. If the absolute deviation of a data point divided by that median is larger than 2, we mark it as a target trigger. The target trigger which is most effective to misclassify the model will be the ``reverse trigger'' we need to repair BadNets.
\end{enumerate}
The implementation of the step 1 and 2 is in the \href{https://github.com/zjzsliyang/CSAW-HackML-2020/blob/master/visualize_example.py}{\texttt{visualize\_example.py}} and \href{https://github.com/zjzsliyang/CSAW-HackML-2020/blob/master/visualizer.py}{\texttt{visualizer.py}}.

The implementation of the step 3 is in the \href{https://github.com/zjzsliyang/CSAW-HackML-2020/blob/master/mad_outlier_detection.py}{\texttt{mad\_outlier\_detection.py}}.

\subsection*{Repair BadNets}
In order to repair BadNets, we decided to patch the infected model by pruning the poisoned neurons in the BadNet with the ``reverse trigger''.

The target trigger poisoned neurons in the model to make it misclassify the label, so we need to find these neurons and set their output value to 0 so that the model will not be affected by the trigger anymore. 

Therefore we rank the neurons by differences between clean input and poisoned input produced by the 'reverse triggers'. We again target the second to last layer, and prune neurons by order of highest rank first. In order to keep the performance of the model on clean target, we decided to stop the iteration as soon as the model is not sensitive to the poisoned input any more.

You can find details in the \href{https://github.com/zjzsliyang/CSAW-HackML-2020/blob/master/repair_model.py}{\texttt{repair\_model.py}}.

\subsection*{Result \& Sample Output}
% [language=bash, caption={bash version}]
\begin{lstlisting}
#!/bin/bash
python3 repair_model.py sunglasses

base model in clean test: 97.77864380358535, poisoned: 99.99220576773187
pruned model in clean test: 86.83554169914264, poisoned: 1.161340607950117
repair model in clean test: 88.08261886204208, fixed poisoned: 100.0
elapsed time 1132.0141394138336 s
\end{lstlisting}

\noindent\rule{2cm}{0.4pt}
\begin{lstlisting}
python3 repair_model.py anonymous_1

base model in clean test: 97.1862821512081, poisoned: 91.3971161340608
pruned model in clean test: 95.12081060015588, poisoned: 3.0982073265783323
repair model in clean test: 79.81293842556508, fixed poisoned: 99.71745908028059
elapsed time 1085.563981294632 s
\end{lstlisting}

\noindent\rule{2cm}{0.4pt}
\begin{lstlisting}
python3 repair_model.py anonymous_2

base model in clean test: 95.96258768511302, poisoned: 0.0
pruned model in clean test: 96.18862042088854, poisoned: 0.03897116134060795
repair model in clean test: 78.95557287607171, fixed poisoned: 99.85385814497272
elapsed time 1077.5421595573425 s
\end{lstlisting}

\noindent\rule{2cm}{0.4pt}
\begin{lstlisting}
python3 repair_model.py multi_trigger_multi_target
base model in clean test: 96.00935307872174, poisoned: 30.452714990906728
pruned model in clean test: 95.86905689789556, poisoned: 1.575084437516238
repair model skipped.
elapsed time 1533.934408903122 s
\end{lstlisting}

\section*{STRIP}
\subsection*{Introduction}
STRIP is a run-time trojan detection system can distinguish trojaned input from clean ones. \cite{gao2019strip} proposed this method and our implementation is based on their work.

\subsection*{Principle}
The principles of STRIP can be illustrated by a example on MNIST handwritten digits. As attack shown in Fig. \ref{fig:strip_1}, the trigger is a square located at the bottom-right corner and the target of the attackers is 7.
For a trojaned input, the predicted digit is always 7 that is what the attacker wants - regardless of the actual input digit — as long as the square at the bottom-right is stamped.

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{img/strip_1.png}
    \caption{Trojan attacks exhibit an input-agnostic behavior. The attacker targeted class is 7.}
    \label{fig:strip_1}
\end{figure}

This input-agnostic characteristic is recognized as main strength of the trojan attack which is exploitable to detect whether a trojan trigger is contained in the input from the perspective of a defender. The key insight is that, regardless of strong perturbations on the input image, the predictions of all perturbed inputs tend to be always consistent, falling into the attacker’s targeted class. This behavior is eventually abnormal and suspicious. Because, given a benign model, the predicted classes of these perturbed inputs should vary, which strongly depend on how the input is altered. Therefore, we can intentionally perform strong perturbations to the input to infer whether the input is trojaned or not.

In Fig. \ref{fig:strip_2}, the input is 8 and is clean. The image linear blend perturbation here is superimposing two images. The digit images to be perturbed with clean input are randomly drawn. Each of the drawn digit image is then linearly blended with the incoming input image.
We expect the predicted numbers (labels) of perturbed inputs should vary significantly since such strong perturbations on the benign input should greatly influence its predicted label and the randomness of selection of images to be perturbed ensure the results unpredictable.

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{img/strip_2.png}
    \caption{This example uses a clean input 8 - $b=8$, $b$ stands for bottom image, the perturbation here is to linearly blend the other digits ($t=5,3,0,7$ from left to right, respectively) that are randomly drawn. Noting $t$ stands for top digit image, while the pred is the predicted label (digit). Predictions are quite different for perturbed clean input 8.}
    \label{fig:strip_2}
\end{figure}

The perturbation strategy work well on clean input. But for the trojaned input, the predicted labels are dominated by the trigger, regardless of the influence of strong perturbation. As shown in Fig. \ref{fig:strip_3}, the prediction are Surprisingly consistent. Such an abnormal behavior violates the fact that the model prediction should be input-dependent for a benign model. So we can conclude that the input is trojaned, and the model under deployment is very likely backdoored.

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{img/strip_3.png}
    \caption{The same input digit 8 as in Fig. \label{fig:strip_2} but stamped with the square trojan trigger is linearly blended the same drawn digits. Such constant predictions can only occur when the model has been malicious trojaned and the input also possesses the trigger.}
    \label{fig:strip_3}
\end{figure}

Fig. \ref{fig:strip_4} depicts the predicted classes' distribution given that 1000 randomly drawn digit images are linearly blended with one given incoming benign and trojaned input, respectively. Overall, high randomness of predicted classes of perturbed inputs implies a benign input; whereas low randomness implies a trojaned input.

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{img/strip_4.png}
    \caption{Prodicted digits' distribution of 1000 perturbed images applied to one given clean/trojaned input image. Inputs of top three sub-figures are trojan-free. Input of bottom sub-figures are trojaned. The attacker targeted class is 7.}
    \label{fig:strip_4}
\end{figure}

\subsection*{Method}
To make the STRIP principle work in practice, we design algorithm as following: 

\begin{algorithm}
  \caption{Run-time detecting trojaned input of the deployed DNN model}
  \begin{algorithmic}
    \Function{detection}{$x, D_{test}, F_\theta(), \text{detection boundary}$}
        \State $\texttt{trojanedFlag} \leftarrow \text{No}$
        \For  {$n=0$ to $N$} 
            \State randomly drawing the $n_{th}$ image, $x_n^t$, from $D_{test}$
            \State produce the $n_{th}$ perturbed images $x^{p_n}$ by superimposing incoming image $x$ with $x_n^t$.
        \EndFor
        \State $H \leftarrow F_\theta (D_p)$
        \If{  $H \leq \text{detection boundary}$ }
            \State $\texttt{trojanedFlag} \leftarrow \text{Yes}$
        \EndIf 
        \State \Return $\texttt{trojanedFlag}$
    \EndFunction
  \end{algorithmic}
\end{algorithm}

$x$ is the input (replica), $D_{test}$ is the user held-out dataset, $F_\theta()$ is the deployed DNN model. According to the input $x$, the DNN model predicts its label $z$. At the same time, the DNN
model determines whether the input $x$ is trojaned or not based on the observation on predicted classes to all N perturbed inputs $\{x^{p_1}, \dotsc, x^{p_N}\}$ that forms a perturbation set $D_p$. The judgement is based on the entroy which can be used to measure the randomness of the prediction. Fig. \ref{fig:strip_5} illustrates the whole process of the STRIP algorithm.

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{img/strip_5.png}
    \caption{Run-time STRIP Trojan Detection System Overview}
    \label{fig:strip_5}
\end{figure}

\subsection*{Entroy}
We consider Shannon entropy to express the randomness of the predicted classes of all perturbed inputs $\{x^{p_1}, \dotsc, x^{p_N}\}$ corresponding to a given incoming input $x$. Starting from the $n^{th}$ perturbed input $x_{p^n} \in \{x^{p_1}, \dotsc, x^{p_N}\}$, its entropy $H_n$ can be expressed:

\begin{equation*}
    H_n = -\sum_{i=1}^M y_i \cdot \log_2 y_i
\end{equation*}

With $y_i$ being the probability of the perturbed input belonging to class $i$. $M$ is the total number of classes. Based on the entropy $H_n$ of each perturbed input $x^{p_n}$, the entropy summation of all $N$ perturbed inputs $\{x^{p_1}, \dotsc, x^{p_N}\}$ is:

\begin{equation*}
    H_{sum} = \sum_{n=1}^N H_n
\end{equation*}

With $H_{sum}$ standing for the chance the input $x$ being trojaned. Higher the $H_{sum}$, lower the probability the input $x$ being a trojaned input.
We further normalize the entropy $H_{sum}$ that is written as:

\begin{equation*}
    H = \frac{1}{N} H_{sum}
\end{equation*}

The $H$ is regarded as the entropy of one incoming input $x$. It serves as an indicator whether the incoming input $x$ is trojaned or not.

% \begin{table}[H]\centering
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{methods} & \textbf{EEG} & \textbf{ECG-40} & \textbf{ECG-15} \\ \hline
% DTW              & 71.6         & 74.5            & 85.5            \\ \hline
% DWT              & 76.0         & 25.1            & 20.1            \\ \hline
% DFT              & 91.6         & 85.6            & 60.6            \\ \hline
% BoW              & 93.8         & 99.5            & 100             \\ \hline
% \end{tabular}
% \caption{Comparison of bag-of-words model}
% \label{table:res}
% \end{table}

% \section*{Concluding Remarks}


\bibliography{ref}
\bibliographystyle{chicago}
\end{document}